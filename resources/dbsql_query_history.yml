resources:
  jobs:
    dbsql_query_history:
      name: dbsql_query_history
      description: Ingests DBSQL query history and writes it to Delta
      tags:
        "workload": "observability"
      parameters:
        - name: catalog
          default: main
        - name: schema
          default: observe
        - name: table
          default: query_history
        - name: pipeline_mode
          default: triggered
        - name: backfill_period
          default: 7 days
        - name: reset
          default: no
      job_clusters:
        - job_cluster_key: default_cluster
          new_cluster:
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            driver_node_type_id: m6gd.xlarge
            aws_attributes:
              availability: SPOT_WITH_FALLBACK
              zone_id: us-west-2a
              spot_bid_price_percent: 100
              first_on_demand: 1
            spark_env_vars:
              DATABRICKS_HOST: https://e2-demo-field-eng.cloud.databricks.com/
              DATABRICKS_TOKEN: '{{secrets/observability/pat}}'
            runtime_engine: STANDARD
            spark_conf:
              "spark.master": "local[*, 4]"
              "spark.databricks.cluster.profile": "singleNode"
            spark_version: 14.3.x-scala2.12
            node_type_id: m6gd.xlarge
            custom_tags:
              "ResourceClass": "SingleNode"
      tasks:
        - task_key: load_query_history
          job_cluster_key: default_cluster
          email_notifications:
            on_failure:
              - chris.koester@databricks.com
          python_wheel_task:
            package_name: dbsql_query_logger
            parameters:
              - "{{job.parameters.catalog}}"
              - "{{job.parameters.schema}}"
              - "{{job.parameters.table}}"
              - "{{job.parameters.pipeline_mode}}"
              - "{{job.parameters.backfill_period}}"
              - "{{job.parameters.reset}}"
            entry_point: main
          libraries:
            # By default we just include the .whl file generated for the dbsql_query_logger package.
            # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
            # for more information on how to add other libraries.
            - whl: ../dist/*.whl
          run_if: ALL_SUCCESS
      max_concurrent_runs: 1